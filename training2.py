# -*- coding: utf-8 -*-
"""Training2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oYuGzFhlRt4w2JjYdWAtMQdcZijtxz8O
"""

from google.colab import drive
drive.mount('/content/drive')

!ls /content/drive/MyDrive

import sys
import subprocess
import torch

# ✅ 1. Check and upgrade transformers if needed
def ensure_transformers_version(min_version="4.4.0"):
    try:
        import transformers
        from packaging import version
        if version.parse(transformers.__version__) < version.parse(min_version):
            print(f"[INFO] Upgrading transformers from {transformers.__version__} to latest...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", "--upgrade", "transformers"])
            print("[INFO] Restart your script after upgrade finishes.")
            sys.exit(0)
    except ImportError:
        print("[INFO] Installing transformers...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", "transformers"])
        print("[INFO] Restart your script after install finishes.")
        sys.exit(0)

ensure_transformers_version()

from datasets import load_from_disk
dataset = load_from_disk("/content/drive/MyDrive/processed_data")

from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments

# ✅ 2. Check GPU availability
if torch.cuda.is_available():
    device_name = torch.cuda.get_device_name(0)
    print(f"🔥 Using GPU: {device_name}")
    device = torch.device("cuda")
else:
    print("⚠️ No GPU detected. Training will run on CPU.")
    device = torch.device("cpu")

# ✅ 3. Load preprocessed dataset
train_dataset = dataset["train"]
test_dataset = dataset["test"]

# ✅ 4. Choose a model & tokenizer
model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token  # Padding fix for GPT-2

# Tokenize function
def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=128
    )

# Apply tokenization
tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=["text"])

train_dataset = tokenized_dataset["train"]
test_dataset = tokenized_dataset["test"]

# ✅ 5. Load the model onto GPU explicitly
model = AutoModelForCausalLM.from_pretrained(model_name).to(device)

if torch.cuda.is_available():
    model = model.to("cuda")
# ✅ 6. Training configurations
training_args = TrainingArguments(
    output_dir="trained_model",
    learning_rate=5e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="logs",
    logging_steps=50,
    save_total_limit=2,
    fp16=True if torch.cuda.is_available() else False,  # Mixed precision on GPU
    push_to_hub=False,
    dataloader_num_workers=0,
    dataloader_pin_memory=True,
)
model.gradient_checkpointing_enable()
# ✅ 7. Data collator
def data_collator(features):
    return {
        "input_ids": torch.nn.utils.rnn.pad_sequence(
            [torch.tensor(f["input_ids"]) for f in features],
            batch_first=True,
            padding_value=tokenizer.pad_token_id
        ),
        "attention_mask": torch.nn.utils.rnn.pad_sequence(
            [torch.tensor(f["attention_mask"]) for f in features],
            batch_first=True,
            padding_value=0
        ),
        "labels": torch.nn.utils.rnn.pad_sequence(
            [torch.tensor(f["input_ids"]) for f in features],
            batch_first=True,
            padding_value=-100
        )
    }

# ✅ 8. Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator
)

# ✅ 9. Manual evaluation after each epoch
num_epochs = int(training_args.num_train_epochs)
for epoch in range(num_epochs):
    print(f"\n===== Epoch {epoch+1}/{num_epochs} =====")
    trainer.train(resume_from_checkpoint=False)
    metrics = trainer.evaluate()
    print(f"📊 Evaluation metrics after epoch {epoch+1}: {metrics}")

# ✅ 10. Save model & tokenizer
model.save_pretrained("trained_model")
tokenizer.save_pretrained("trained_model")
print("✅ Training complete! Model saved in 'trained_model/'")

# Replace with the path you want inside Drive
save_directory = "/content/drive/MyDrive/my_trained_model"

# Save model, tokenizer, and training args
model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

print(f"✅ Model saved to {save_directory}")
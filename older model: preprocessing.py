# -*- coding: utf-8 -*-
"""Preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ARMqnJxCaKmPtNdDCgP3c26IMXSwZXDj
"""

import re
from transformers import AutoTokenizer
from datasets import load_dataset

dataset = load_dataset("wikitext", "wikitext-2-v1", trust_remote_code=True)

print(dataset)

def clean_text(text):
    text = text.strip()  # remove leading/trailing spaces
    text = re.sub(r"\s+", " ", text)  # collapse multiple spaces/newlines
    text = re.sub(r"[^\x00-\x7F]+", " ", text)  # remove non-ASCII if needed
    return text

dataset = dataset.map(lambda x: {"text": clean_text(x["text"])})

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# Add pad token if it doesn't exist
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    tokenizer.pad_token = tokenizer.eos_token  # or use '[PAD]' if you prefer

def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=128
    )

tokenized_dataset = dataset.map(tokenize_function, batched=True)

split_dataset = tokenized_dataset["train"].train_test_split(test_size=0.1)
train_data = split_dataset["train"]
test_data = split_dataset["test"]

tokenized_dataset.save_to_disk("processed_data")

from google.colab import drive
drive.mount('/content/drive')
dataset.save_to_disk("/content/drive/MyDrive/processed_data")

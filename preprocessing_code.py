# -*- coding: utf-8 -*-
"""Preprocessing_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e2QW-9pMPY1XiCtEjUMWcyX_4tWIt0si

Welcome to the preprocessing section of my project! Here, I clean my data and make it easier for the AI to interpret. Immediatly below, I have the main dataset I used, a compilation of therapeutic conversations between people. It was an improvement from my original dataset, which was a massive compilation of therapists' conversations, and very hard to implement in my AI. So, I used this dataset, which is much easier to implement. But, it does have some slang and speaks quite interestingly, to say the least, which I guess is a trade-off for ease of implementation :) .
"""

import re
from transformers import AutoTokenizer
from datasets import load_dataset

dataset = load_dataset("chill-bots/empathetic-dialogues-cleaned")

print(dataset)

def clean_text(text):
    text = re.sub(r"\s+", " ", text.strip())
    text = re.sub(r"http\S+", "", text)
    text = text.replace("’", "'").replace("“", '"').replace("”", '"')
    return text

def combine_fields(example):
    messages = example["messages"]
    lines = []
    for m in messages:
      role = m["role"]
      content = clean_text(m["content"])
      lines.append(f"{role}: {content}")
    combined = "\n".join(lines)
    return {"text": combined}

dataset = dataset.map(combine_fields)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("gpt2-medium")

# Add pad token if it doesn't exist
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    tokenizer.pad_token = tokenizer.eos_token  # or use '[PAD]' if you prefer

def tokenize_function(examples):
    # assuming you have combined text in "text" column
    tokenized = tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=128
    )
    # causal LM needs labels
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

tokenized_dataset = dataset.map(tokenize_function, batched=True)

split_dataset = tokenized_dataset["train"].train_test_split(test_size=0.1)
train_data = split_dataset["train"]
test_data = split_dataset["test"]

tokenized_dataset.save_to_disk("processed_data")

print(dataset["train"][0]["text"])

from google.colab import drive
drive.mount('/content/drive')
dataset.save_to_disk("/content/drive/MyDrive/Attreya_Maker_Portfolio-rasA/Data-ONLY_FOR_VIEWING/processed_data-empathetic")

"""I wanted some fine-tuned emotion recognition in my AI, so I decided to have the dataset below just as a next step. Feel free to look at the difference between the preprocessing codes!"""

import re
from transformers import AutoTokenizer
from datasets import load_dataset

dataset = load_dataset("bdotloh/empathetic-dialogues-contexts")

print(dataset)

def clean_text(text):
    text = re.sub(r"\s+", " ", text.strip())
    text = re.sub(r"http\S+", "", text)
    text = text.replace("’", "'").replace("“", '"').replace("”", '"')
    return text

def combine_fields(example):
    context = clean_text(example["situation"])
    response = clean_text(example["emotion"])
    combined = f"User: {context}\nAssistant: {response}"
    return {"text": combined}

dataset = dataset.map(combine_fields)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("gpt2-medium")

# Add pad token if it doesn't exist
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    tokenizer.pad_token = tokenizer.eos_token  # or use '[PAD]' if you prefer

def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=128
    )
    tokenized["labels"] = tokenized["input_ids"].copy()  # add labels
    return tokenized

tokenized_dataset = dataset.map(tokenize_function, batched=True)

split_dataset = tokenized_dataset["train"].train_test_split(test_size=0.1)
train_data = split_dataset["train"]
test_data = split_dataset["test"]

tokenized_dataset.save_to_disk("processed_data")

print(dataset["train"][0]["text"])

from google.colab import drive
drive.mount('/content/drive')
dataset.save_to_disk("/content/drive/MyDrive/Attreya_Maker_Portfolio-rasA/Data - ONLY FOR VIEWING/processed_data-emotion")
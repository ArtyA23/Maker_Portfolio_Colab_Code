# -*- coding: utf-8 -*-
"""Initial_Model_Testing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fHQchK1RCD7JdeN32i8Ir5OwLS6OAlyU
"""

from google.colab import drive
drive.mount('/content/drive')

"""**For an actual chat-like experience:**"""

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_name = "drive/MyDrive/Attreya_AI_project/my_trained_model"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Make sure pad token exists for GPT-2 style models
tokenizer.pad_token = tokenizer.eos_token
model.eval()  # turn off train-time layers like dropout

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

@anvil.server.callable
def chat_with_model(prompt, max_new_tokens=100, do_sample=True, temperature=1.0, top_p=1.0):
    # Tokenize input and send tensors to the model device
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True,
                       max_length=tokenizer.model_max_length).to(device)
    input_len = inputs["input_ids"].shape[1]

    # Generate tokens (the output contains both prompt + new tokens)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=do_sample,
            top_p=top_p,
            temperature=temperature,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )

    # Remove the prompt tokens and decode only the newly generated tokens
    generated_ids = outputs[0, input_len:]
    reply = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
    return reply

# Example
prompt = """hello"""
print(chat_with_model(prompt))

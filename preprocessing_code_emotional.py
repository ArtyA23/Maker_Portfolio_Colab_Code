# -*- coding: utf-8 -*-
"""Preprocessing_code-emotional.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aU0HbW2af1ez45RTeukshaj6DknzcFpM

Here, I have a dataset I used that I considered very important - my emotional dataset. This attempts to give my AI some therapeutic knowledge, not to replace therapy, but to mimic it at a small scale. This really gives my AI that human touch, making it more understanding than other models.
"""

import re
from transformers import AutoTokenizer
from datasets import load_dataset
from datasets import Dataset

raw = load_dataset("thu-coai/esconv")

print(dataset['train'][0])

def clean(text):
    text = re.sub(r"\s+", " ", text.strip())
    text = re.sub(r"http\S+", "", text)
    text = text.replace("’", "'").replace("“", '"').replace("”", '"')
    return text

def convo_to_pairs(example, role_user_set=None, role_assistant_set=None):
    if role_user_set is None:
        role_user_set = {"seeker", "user", "client", "asker"}
    if role_assistant_set is None:
        role_assistant_set = {"supporter", "helper", "responder"}

    msgs = example.get("text") or example.get("messages") or []

    turns = []
    for m in msgs:
        if isinstance(m, dict):
            role = (m.get("role") or m.get("speaker") or "").strip().lower() or "speaker"

            if role in ("helper", "responder", "supporter"):
                role = "supporter"
            if role in ("user", "seeker", "client", "asker"):
                role = "seeker"
            utter = clean(m.get("text") or m.get("utterance") or m.get("content") or "")
            turns.append(f"{role}: {utter}")
        elif isinstance(m, str):
            turns.append(clean(m))
        else:
            turns.append(clean(str(m)))

    pairs = []

    for i in range(1, len(turns)):
        prev_role = turns[i-1].split(":", 1)[0].strip().lower()
        curr_role = turns[i].split(":", 1)[0].strip().lower()
        if prev_role in role_user_set and curr_role in role_assistant_set:
            context = " \n".join(turns[:i])
            response = turns[i].split(":", 1)[1].strip()
            pairs.append({"context": context, "response": response})
    return {"pairs": pairs}

raw = raw.map(convo_to_pairs)

pair_rows = []
for split in raw.keys():
    for ex in raw[split]:
        for p in ex.get("pairs", []):
            pair_rows.append(p)

pairs_ds = Dataset.from_list(pair_rows)

tokenizer = AutoTokenizer.from_pretrained("gpt2-medium")

pad_id = tokenizer.pad_token_id

def tokenize_and_mask(batch):
    input_ids_batch = []
    attention_masks = []
    labels_batch = []

    for ctx, resp in zip(batch["context"], batch["response"]):
        ctx_ids = tokenizer(ctx, add_special_tokens=False)["input_ids"]
        resp_ids = tokenizer(resp, add_special_tokens=False)["input_ids"]

        input_ids = ctx_ids + resp_ids
        labels = [-100] * len(ctx_ids) + resp_ids

        if len(input_ids) > MAX_LENGTH:
            trim = len(input_ids) - MAX_LENGTH
            input_ids = input_ids[trim:]
            labels = labels[trim:]

        pad_len = MAX_LENGTH - len(input_ids)
        if pad_len > 0:
            input_ids = input_ids + [pad_id] * pad_len
            labels = labels + [-100] * pad_len

        attn = [1 if tid != pad_id else 0 for tid in input_ids]

        input_ids_batch.append(input_ids)
        attention_masks.append(attn)
        labels_batch.append(labels)

    return {"input_ids": input_ids_batch, "attention_mask": attention_masks, "labels": labels_batch}

tokenized_pairs = pairs_ds.map(tokenize_and_mask, batched=True, remove_columns=pairs_ds.column_names)

example = tokenized_pairs[0]
print("input_ids len:", len(example["input_ids"]))
print("labels sample (first 50):", example["labels"][:50])
first_label_idx = next((i for i, v in enumerate(example["labels"]) if v != -100), None)
print("first non-masked token index:", first_label_idx)
print("decoded tail (response tokens):", tokenizer.decode(example["input_ids"][first_label_idx:]).strip())

tokenized_pairs.save_to_disk("processed_data-emotional")

from google.colab import drive
drive.mount('/content/drive')
da.save_to_disk("/content/drive/MyDrive/Attreya_AI_Colab_Project/Attreya_Maker_Portfolio-rasA/Data-ONLY_FOR_VIEWING/processed_data-emotional")

import os
model_path = "/content/drive/MyDrive/Attreya_AI_Colab_Project/Attreya_Maker_Portfolio-rasA/Data-ONLY_FOR_VIEWING/processed_data-emotional"
print(os.path.exists(model_path))
print(os.listdir(model_path))